{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c00d2f9",
   "metadata": {},
   "source": [
    "# Phase 1: Problem Definition\n",
    "\n",
    "**Topic**: Optimal Item-Level Discount Targeting for Churn Prevention\n",
    "\n",
    "**Course**: Stanford CME 241 – Winter 2026\n",
    "\n",
    "**Date**: February 9, 2026\n",
    "\n",
    "## Problem Description and Relevance\n",
    "\n",
    "In the online retail economy, customer retention is often prioritized over short-term revenue. However, aggressive retention strategies, such as frequent deep discounting, can lead to **Discount Addiction** (formally known as the Reference Price Effect). When customers receive frequent price cuts, their internal reference price lowers, and they may eventually refuse to purchase at full price, eroding long-term customer Lifetime Value (LTV).\n",
    "\n",
    "### Design Constraint: Fixed Discount Depth\n",
    "\n",
    "To reduce the complexity of the action space and limit regulatory exposure from fully personalized pricing, we fix the discount depth to a constant $\\delta$ (e.g., 30%) for all promotions. The agent's only decision is *which product to promote* (if any) — not how large the discount should be. This shifts the optimization problem from *how much to charge* to *what item to feature*.\n",
    "\n",
    "### Practical Challenge\n",
    "\n",
    "We aim to build a recommendation engine that balances three conflicting objectives:\n",
    "\n",
    "- **Churn Risk**: Offering a discount on a high-affinity item reduces the probability of the customer leaving (terminal state). Preventing churn is the key driver, as active customers continue to engage and generate revenue.\n",
    "- **Discount Addiction**: Frequent discounts lower the customer's internal reference price. Discount too often and the customer refuses to buy at full price, permanently eroding margins.\n",
    "- **Cannibalization**: Discounting an item the customer would have bought at full price wastes margin without generating an incremental sale.\n",
    "\n",
    "### Data\n",
    "\n",
    "We use the *Dunnhumby — The Complete Journey* dataset [1] to calibrate the model parameters:\n",
    "\n",
    "> This dataset contains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a retailer. It contains all of each household's purchases, not just those from a limited number of categories.\n",
    "\n",
    "We cluster products into $N$ categories to define a tractable action space. We assume products in the same category are perfect substitutes and the probability of purchase for products in different categories are perfectly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6d9ed",
   "metadata": {},
   "source": [
    "## MDP Formulation\n",
    "\n",
    "We model the problem as a discounted, infinite-horizon MDP $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$. At each discrete time step $t = 0, 1, 2, \\ldots$ the agent (retailer) observes one customer's state, selects a product to promote (or nothing), and collects revenue. The episode ends when the customer churns. The discount depth on every promoted item is fixed at $\\delta$ (e.g. 0.30).\n",
    "\n",
    "### State Space $\\mathcal{S}$\n",
    "\n",
    "In full generality, the state at time $t$ would comprise user-level features $U_t \\in \\mathbb{R}^{D_1}$ (demographics, engagement history), product features $P_t \\in \\mathbb{R}^{N \\times D_2}$ (price, category), and user–product interaction features $X_t \\in \\mathbb{R}^{N \\times D_3}$ (purchase counts, affinity scores). For tractability we retain three core components:\n",
    "\n",
    "$$s_t = (c_t,\\; \\mathbf{m}_t,\\; \\mathbf{l}_t) \\quad \\in \\quad \\mathcal{S} \\cup \\{s_{\\varnothing}\\}$$\n",
    "\n",
    "| Component | Domain | Meaning |\n",
    "|-----------|--------|---------|\n",
    "| $c_t$ | $[0,1]$ | **Churn propensity**: a scalar summarizing overall disengagement risk. Evolves based on customer's recent purchase history and engagement level. |\n",
    "| $m_t^{(i)}$ | $[0, \\infty)$ | **Discount memory** for product $i$: an exponentially weighted accumulation of past discounts. Higher values mean the customer's reference price has been distorted downward. |\n",
    "| $l_t^{(i)}$ | $\\mathbb{Z}_{\\ge 0}$ | **Recency**: number of periods since the customer last purchased product $i$. |\n",
    "\n",
    "Here $\\mathbf{m}_t = (m_t^{(1)}, \\ldots, m_t^{(N)})$ and $\\mathbf{l}_t = (l_t^{(1)}, \\ldots, l_t^{(N)})$, giving a **live-state** dimension of $1 + 2N$.\n",
    "\n",
    "**Terminal state** $s_{\\varnothing}$: An absorbing state representing a churned customer. Once $s_t = s_{\\varnothing}$, the customer is permanently lost: no further purchases occur and $R(s_{\\varnothing}, \\cdot) = 0$ for all actions. The episode ends.\n",
    "\n",
    "### Action Space $\\mathcal{A}$\n",
    "\n",
    "$$a_t \\in \\mathcal{A} = \\{0, 1, 2, \\ldots, N\\}$$\n",
    "\n",
    "$a_t = 0$ means no promotion. $a_t = i$ means promote product $i$ at a discounted price $(1 - \\delta) \\cdot p_i$.\n",
    "\n",
    "### Transition Dynamics $P$\n",
    "\n",
    "The state evolves via the following mechanisms:\n",
    "\n",
    "**1. Effective price.** The price the customer faces for product $j$:\n",
    "\n",
    "$$\\tilde{p}_j(a_t) = p_j \\cdot \\bigl(1 - \\delta \\cdot \\mathbb{1}[a_t = j]\\bigr)$$\n",
    "\n",
    "**2. Reference price and perceived deal.** Past discounts erode what the customer considers a \"fair\" price. The reference price for product $j$ is:\n",
    "\n",
    "$$r_t^{(j)} = p_j - \\beta_m \\cdot m_t^{(j)}$$\n",
    "\n",
    "The customer's purchase decision is driven by the *perceived deal* — the gap between their reference price and the actual price:\n",
    "\n",
    "$$d_t^{(j)} = r_t^{(j)} - \\tilde{p}_j(a_t)$$\n",
    "\n",
    "When $d > 0$ the customer feels they are getting a bargain. When $d < 0$ the price exceeds expectations and the customer is reluctant to buy.\n",
    "\n",
    "**3. Purchase probability.** Each product is purchased independently via a logistic model:\n",
    "\n",
    "$$P(\\text{buy}_j \\mid s_t, a_t) = \\sigma\\!\\bigl(\\,\\beta_0^{(j)} + \\beta_p \\cdot d_t^{(j)} - \\beta_l \\cdot l_t^{(j)}\\bigr)$$\n",
    "\n",
    "| Parameter | Role |\n",
    "|-----------|------|\n",
    "| $\\beta_0^{(j)}$ | Baseline popularity of product $j$ |\n",
    "| $\\beta_p > 0$ | Price sensitivity (how strongly the perceived deal drives purchase) |\n",
    "| $\\beta_m > 0$ | Memory sensitivity (how much past discounts distort the reference price) |\n",
    "| $\\beta_l > 0$ | Recency decay (products not bought recently are harder to sell) |\n",
    "\n",
    "**4. Memory update.**\n",
    "\n",
    "$$m_{t+1}^{(i)} = \\alpha \\, m_t^{(i)} + \\mathbb{1}[a_t = i] \\cdot \\delta, \\qquad \\alpha \\in [0,1)$$\n",
    "\n",
    "Memory decays at rate $\\alpha$ each period and receives a $\\delta$ boost whenever the product is promoted.\n",
    "\n",
    "**5. Recency update.**\n",
    "\n",
    "$$l_{t+1}^{(i)} = \\begin{cases} 0 & \\text{if product } i \\text{ was purchased at } t \\\\ l_t^{(i)} + 1 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**6. Churn propensity update.** The scalar $c_t$ captures global disengagement and evolves based on whether the customer made *any* purchase this period:\n",
    "\n",
    "$$c_{t+1} = \\begin{cases} \\max(c_t - \\eta,\\; 0) & \\text{if any product was purchased at } t \\\\ \\min(c_t + \\eta,\\; 1) & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "where $\\eta > 0$ controls how quickly engagement builds or erodes. Purchases reduce churn risk; idle periods increase it.\n",
    "\n",
    "**7. Churn (transition to terminal state).** At the end of each step, the customer churns with probability:\n",
    "\n",
    "$$P(s_{t+1} = s_{\\varnothing} \\mid s_t, a_t) = c_t \\cdot \\prod_{j=1}^{N}\\bigl(1 - P(\\text{buy}_j \\mid s_t, a_t)\\bigr)$$\n",
    "\n",
    "Churn probability is the product of the customer's current disengagement level $c_t$ and the probability that they bought *nothing* this period. If the customer churns, the state transitions to the absorbing terminal state $s_{\\varnothing}$ and the episode ends with all future rewards equal to zero.\n",
    "\n",
    "### Reward Function $R$\n",
    "\n",
    "$$E[R(s_t, a_t)] = \\begin{cases} \\displaystyle\\sum_{j=1}^{N} P(\\text{buy}_j \\mid s_t, a_t) \\cdot \\tilde{p}_j(a_t) & \\text{if } s_t \\neq s_{\\varnothing} \\\\[6pt] 0 & \\text{if } s_t = s_{\\varnothing} \\end{cases}$$\n",
    "\n",
    "For live states, the immediate reward is expected revenue. Discounting has two distinct immediate costs:\n",
    "- **Cannibalization**: If the customer would have bought the item anyway at full price, the discount is pure waste — we sacrifice $\\delta \\cdot p_i$ in margin for a sale that would have occurred regardless.\n",
    "- **Revenue dilution**: Even when the discount *does* cause an incremental purchase (one that wouldn't have happened at full price), we only earn $(1-\\delta) \\cdot p_i$ per unit instead of $p_i$.\n",
    "\n",
    "The long-term cost — **discount addiction** — does not appear in $R$ directly. It flows through the state transition: promoting product $i$ raises $m_{t+1}^{(i)}$, which lowers the future reference price $r_{t+1}^{(i)}$, which reduces future full-price purchase probability. Similarly, **churn** is penalized indirectly: allowing $c_t$ to rise increases the probability of transitioning to $s_{\\varnothing}$, which zeroes out all future revenue.\n",
    "\n",
    "### Discount Factor and Objective\n",
    "\n",
    "We use $\\gamma \\in (0,1)$ (e.g. $\\gamma = 0.99$). The objective is:\n",
    "\n",
    "$$\\max_{\\pi}\\; \\mathbb{E}_{\\pi}\\!\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\, R(s_t, a_t)\\right]$$\n",
    "\n",
    "Combined with stochastic churn, $\\gamma$ gives an effective planning horizon that depends on customer engagement. A low-churn customer has a longer horizon, making the long-term addiction cost more salient to the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cd9d4",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "To build intuition, consider a store with $N = 2$ products and these parameters:\n",
    "\n",
    "**Model parameters:**\n",
    "\n",
    "| | Coffee ($j=1$) | Tea ($j=2$) |\n",
    "|--|:--:|:--:|\n",
    "| Shelf price $p_j$ | \\$10 | \\$5 |\n",
    "| Baseline $\\beta_0^{(j)}$ | 0.5 | 0.3 |\n",
    "\n",
    "$\\delta = 0.30$, $\\alpha = 0.90$, $\\beta_m = 2$, $\\beta_p = 0.5$, $\\beta_l = 0.05$, $\\eta = 0.05$, $\\gamma = 0.99$\n",
    "\n",
    "**Initial state:**\n",
    "\n",
    "| | Coffee ($j=1$) | Tea ($j=2$) |\n",
    "|--|:--:|:--:|\n",
    "| Memory $m_0^{(j)}$ | 1.5 (many past discounts) | 0.0 (never discounted) |\n",
    "| Recency $l_0^{(j)}$ | 2 periods | 8 periods |\n",
    "\n",
    "$c_0 = 0.05$\n",
    "\n",
    "---\n",
    "\n",
    "**Suppose the agent promotes Coffee** ($a_0 = 1$):\n",
    "\n",
    "**Step 1 — Effective prices:**\n",
    "- Coffee: $\\tilde{p}_1 = 10 \\times (1 - 0.30) = \\$7.00$\n",
    "- Tea: $\\tilde{p}_2 = \\$5.00$ (no discount)\n",
    "\n",
    "**Step 2 — Reference prices:**\n",
    "- Coffee: $r_1 = 10 - 2 \\times 1.5 = \\$7.00$ (eroded by past discounts)\n",
    "- Tea: $r_2 = 5 - 2 \\times 0 = \\$5.00$ (undistorted)\n",
    "\n",
    "**Step 3 — Perceived deals:**\n",
    "- Coffee: $d_1 = 7 - 7 = 0$ (discount merely restores \"neutral\" — no real bargain)\n",
    "- Tea: $d_2 = 5 - 5 = 0$ (also neutral at full price)\n",
    "\n",
    "**Step 4 — Purchase probabilities:**\n",
    "- Coffee: $\\sigma(0.5 + 0.5 \\times 0 - 0.05 \\times 2) = \\sigma(0.4) \\approx 0.60$\n",
    "- Tea: $\\sigma(0.3 + 0.5 \\times 0 - 0.05 \\times 8) = \\sigma(-0.1) \\approx 0.48$\n",
    "\n",
    "**Step 5 - Env update:**\n",
    "- $P(\\text{churn}) = 0.05 \\times (1 - 0.60) \\times (1 - 0.48) \\approx 1\\%$\n",
    "- Suppose the customer buys coffee and does not churn\n",
    "- $c_{1} = \\max(c_0 - \\eta, 0) = \\max(0.05 - 0.05, 0) = 0$\n",
    "\n",
    "**Step 6 — Reward:**\n",
    "- Expected revenue: $E(R_0) = \\$7.00 \\times 0.60 + \\$5.00 \\times 0.48 \\approx \\$6.60$\n",
    "\n",
    "**Step 7 — State transition:**\n",
    "- Coffee memory: $m_{1}^{(1)} = 0.9 \\times 1.5 + 0.30 = 1.65$ *(addiction deepens)*\n",
    "- Tea memory: $m_{1}^{(2)} = 0.9 \\times 0 = 0.00$\n",
    "- Coffee recency: $l_1^{(1)}= 0$ *(product was purchased)*\n",
    "- Tea recency: $l_1^{(2)}= l_0^{(2)} + 1 = 9$ *(product was not purchased)*\n",
    "\n",
    "$s_1 = (c_1,\\; \\mathbf{m}_1,\\; \\mathbf{l}_1) = (0.0,\\; (1.65,\\; 0.0),\\; (0,\\; 9))$\n",
    "\n",
    "---\n",
    "\n",
    "**Now suppose instead the agent promotes Tea** ($a_0 = 2$):\n",
    "\n",
    "| | Coffee | Tea |\n",
    "|--|:--:|:--:|\n",
    "| Effective price | $10.00 | 5 \\times (1 - 0.30) = \\$3.50 |\n",
    "| Reference price | 10 - 2 \\times 1.5 = \\$7.00 | 5 - 2 \\times 0 = \\$5.00 |\n",
    "| Perceived deal | 7.00 - 10 = -3.00 | 5 - 3.50 = +1.50 |\n",
    "| $P(\\text{buy})$ | $\\sigma(0.5 - 1.5 - 0.1) = \\sigma(-1.1) \\approx 0.25$ | $\\sigma(0.3 + 0.75 - 0.4) = \\sigma(0.65) \\approx 0.66$ |\n",
    "\n",
    "$E(R_1) = \\$3.50 \\times 0.66 + 10 \\times 0.25 \\approx \\$4.81$\n",
    "\n",
    "**Key insight:** Promoting Coffee earned more immediate expected revenue (\\$6.60 vs \\$4.81), but it deepened Coffee addiction ($m_1^{(1)} \\to 1.65$). Promoting Tea sacrifices short-term revenue, but started building a new relationship and earned a genuine \"deal\" lift. Meanwhile, Coffee at full price had $d = -3$ — drastically reducing purchase probability at full price.\n",
    "\n",
    "This is the fundamental trade-off the optimal policy must navigate: short-term revenue vs. long-term reference-price erosion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de7c0c",
   "metadata": {},
   "source": [
    "## Problem Versions\n",
    "\n",
    "We define three versions of increasing complexity per the CME 241 project guidelines.\n",
    "\n",
    "### Version A: Ideal / Commercial\n",
    "\n",
    "The full problem with no simplifications:\n",
    "- **Products:** Full catalog ($N \\sim 100{+}$ SKUs), continuous prices.\n",
    "- **State:** Rich user and product features (demographics, engagement history, purchase history, cross-product affinities), real-time seasonality, cost models.\n",
    "- **Transitions:** Learned from production transaction logs; non-stationary (customer preferences drift).\n",
    "- **Action:** Relax the fixed-$\\delta$ constraint to include regional or time-varying discount depths.\n",
    "- **Scale:** Millions of customers, personalized policies per user segment.\n",
    "\n",
    "This version requires a production ML pipeline and is beyond the scope of the course.\n",
    "\n",
    "### Version B: Phase 3 Target (RL)\n",
    "\n",
    "A realistic version solvable with function approximation RL:\n",
    "- **Products:** $N \\approx 5$ product categories (clustered from Dunnhumby data).\n",
    "- **State:** Continuous $(c_t, \\mathbf{m}_t, \\mathbf{l}_t)$.\n",
    "- **Action:** $\\mathcal{A} = \\{0, 1, \\ldots, 5\\}$ (promote one category or nothing).\n",
    "- **Transitions:** Simulated customer; the model-free agent does **not** have access to $P$.\n",
    "\n",
    "| Parameter | Source | Details |\n",
    "|--|:--:|--|\n",
    "| $\\delta,\\; \\gamma$ | Fixed | Design choices ($\\delta = 0.30$, $\\gamma = 0.99$) |\n",
    "| $p_j,\\; \\beta_0^{(j)}$ | Calibrated | Per-category prices and baselines fitted from Dunnhumby data |\n",
    "| $\\beta_p,\\; \\beta_m,\\; \\beta_l$ | Calibrated | Demand-model coefficients estimated from purchase history data |\n",
    "| $\\alpha,\\; \\eta$ | Calibrated | Behavioral dynamics estimated from data |\n",
    "| $\\pi^*$ | **Learned** | Policy discovered via RL |\n",
    "\n",
    "### Version C: Phase 2 Target (DP)\n",
    "\n",
    "A simplified version solvable via tabular Dynamic Programming:\n",
    "- **Products:** $N \\leq 5$ product categories (we use $N = 2$ for the worked example). Capping $N$ keeps the $2^N$ purchase-subset enumeration tractable ($2^5 = 32$ outcomes per action).\n",
    "- **State:** Discretized - $c_t \\in \\{\\text{Low}, \\text{High}\\}$, $m_t^{(i)} \\in \\{\\text{Low}, \\text{Med}, \\text{High}\\}$, $l_t^{(i)} \\in \\{\\text{Recent}, \\text{Stale}\\}$ where \"Stale\" captures $l_t$ being greater than some threshold, giving $|\\mathcal{S}| = 2 \\times 3^N \\times 2^N$ states (72 for $N=2$).\n",
    "- **Action:** $\\mathcal{A} = \\{0, 1, \\ldots, N\\}$.\n",
    "- **Purchase model:** Same independent logistic model as the general formulation. Each product $j$ is bought independently with probability $\\sigma(u_j)$, so every subset $B \\subseteq \\{1,\\ldots,N\\}$ is a possible outcome. This yields at most $2^N + 1$ transition branches per $(s, a)$ pair (one per purchase subset, plus churn when $B = \\varnothing$).\n",
    "- **Transitions:** Tabular $P(s' \\mid s, a)$ assumed known and static. Computed **analytically** from the demand model.\n",
    "\n",
    "| Parameter | Source | Details |\n",
    "|--|:--:|--|\n",
    "| All model params | Fixed | Hand-chosen values (as in the worked example above) |\n",
    "| $P(s' \\mid s, a)$ | **Computed** | Full transition matrix derived analytically from the demand model |\n",
    "| $\\pi^*$ | **Computed** | Exact solution via Value Iteration |\n",
    "\n",
    "This version is small enough for exact computation while still exhibiting the core churn-vs-addiction trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd7d59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76385308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from itertools import product as cartesian_product\n",
    "from typing import FrozenSet, Iterable, Tuple\n",
    "\n",
    "from rl.distribution import Categorical, Distribution\n",
    "from rl.markov_process import NonTerminal, Terminal, State\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Phase 2 (Version C): Tabular DP MDP for Discount Targeting\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DiscountState:\n",
    "    \"\"\"Discretized customer state for the DP version.\n",
    "\n",
    "    Attributes:\n",
    "        churn_propensity: Index into C_GRID  (0 = Low, 1 = High)\n",
    "        discount_memory:  Tuple of per-product indices into M_GRID\n",
    "                          (0 = Low, 1 = Med, 2 = High)\n",
    "        purchase_recency: Tuple of per-product indices into L_GRID\n",
    "                          (0 = Recent, 1 = Stale)\n",
    "    \"\"\"\n",
    "    churn_propensity: int\n",
    "    discount_memory: Tuple[int, ...]\n",
    "    purchase_recency: Tuple[int, ...]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Product:\n",
    "    id: int\n",
    "    price: float\n",
    "    baseline_demand: float\n",
    "\n",
    "\n",
    "class DiscountDP(MarkovDecisionProcess[DiscountState, int]):\n",
    "    \"\"\"Version C MDP: independent-purchase model, discretized state.\n",
    "\n",
    "    Each product is purchased independently (logistic probability), so\n",
    "    every subset of products is a possible outcome.  We cap N <= 5 to\n",
    "    keep the 2^N subset enumeration tractable (at most 32 outcomes per\n",
    "    action).  Churn occurs only when no product is purchased.\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_N = 5  # keeps 2^N enumeration tractable\n",
    "\n",
    "    def __init__(self, products: list[Product]):\n",
    "        self.N: int = len(products)\n",
    "        assert self.N <= self.MAX_N, \\\n",
    "            f\"N={self.N} exceeds MAX_N={self.MAX_N}; \" \\\n",
    "            f\"2^N={2**self.N} purchase subsets would be too large for tabular DP\"\n",
    "\n",
    "        # Model parameters (hand-chosen for Phase 2)\n",
    "        self.delta: float = 0.30        # Fixed discount depth\n",
    "        self.gamma: float = 0.99        # MDP discount factor\n",
    "        self.beta_p: float = 0.5        # Price sensitivity\n",
    "        self.beta_m: float = 2.0        # Memory sensitivity\n",
    "        self.beta_l: float = 0.05       # Recency decay\n",
    "        self.alpha: float = 0.9         # Memory decay rate\n",
    "        self.eta: float = 0.05          # Churn volatility\n",
    "\n",
    "        # Per-product parameters (0-indexed)\n",
    "        self.prices = [p.price for p in products]\n",
    "        self.baselines = [p.baseline_demand for p in products]\n",
    "\n",
    "        # Discretization grids\n",
    "        self.C_GRID = [0.05, 0.50]       # Low / High churn\n",
    "        self.M_GRID = [0.0, 1.0, 2.0]    # Low / Med / High memory\n",
    "        self.L_GRID = [0, 5]             # Recent / Stale\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # MarkovDecisionProcess interface\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def actions(self, state: NonTerminal[DiscountState]) -> Iterable[int]:\n",
    "        \"\"\"0 = no promotion, 1..N = promote product j.\"\"\"\n",
    "        return list(range(self.N + 1))\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[DiscountState],\n",
    "        action: int,\n",
    "    ) -> Distribution[Tuple[State[DiscountState], float]]:\n",
    "        s: DiscountState = state.state          # unwrap inner state\n",
    "\n",
    "        # 1. Decode discretized indices → continuous values\n",
    "        c_val = self.C_GRID[s.churn_propensity]\n",
    "        m_vals = [self.M_GRID[s.discount_memory[j]] for j in range(self.N)]\n",
    "        l_vals = [self.L_GRID[s.purchase_recency[j]] for j in range(self.N)]\n",
    "\n",
    "        # 2. Compute per-product buy probability and effective price\n",
    "        buy_probs: list[float] = []\n",
    "        eff_prices: list[float] = []\n",
    "\n",
    "        for j in range(self.N):\n",
    "            is_promoted = (action == j + 1)     # actions are 1-indexed\n",
    "            eff_p = self.prices[j] * (1.0 - self.delta) if is_promoted \\\n",
    "                else self.prices[j]\n",
    "            eff_prices.append(eff_p)\n",
    "\n",
    "            ref_price = self.prices[j] - self.beta_m * m_vals[j]\n",
    "            deal = ref_price - eff_p\n",
    "\n",
    "            logit = self.baselines[j] + self.beta_p * deal \\\n",
    "                - self.beta_l * l_vals[j]\n",
    "            buy_probs.append(1.0 / (1.0 + np.exp(-logit)))\n",
    "\n",
    "        # 3. Enumerate all 2^N purchase subsets\n",
    "        outcomes: dict[Tuple[State[DiscountState], float], float] = {}\n",
    "\n",
    "        for bought_vec in cartesian_product([False, True], repeat=self.N):\n",
    "            # Joint probability of this particular buy/no-buy combination\n",
    "            prob = 1.0\n",
    "            for j in range(self.N):\n",
    "                prob *= buy_probs[j] if bought_vec[j] else (1.0 - buy_probs[j])\n",
    "            if prob <= 0:\n",
    "                continue\n",
    "\n",
    "            purchased: FrozenSet[int] = frozenset(\n",
    "                j for j in range(self.N) if bought_vec[j]\n",
    "            )\n",
    "            reward = sum(eff_prices[j] for j in purchased)\n",
    "\n",
    "            if len(purchased) > 0:\n",
    "                # At least one product bought → no churn\n",
    "                next_s = self._next_state(s, action, purchased)\n",
    "                key = (NonTerminal(next_s), reward)\n",
    "                outcomes[key] = outcomes.get(key, 0.0) + prob\n",
    "            else:\n",
    "                # Nothing bought → churn with probability c_val\n",
    "                p_churn = prob * c_val\n",
    "                if p_churn > 0:\n",
    "                    outcomes[(Terminal(s), 0.0)] = \\\n",
    "                        outcomes.get((Terminal(s), 0.0), 0.0) + p_churn\n",
    "\n",
    "                # Nothing bought → survive\n",
    "                p_survive = prob * (1.0 - c_val)\n",
    "                if p_survive > 0:\n",
    "                    next_s = self._next_state(s, action, purchased)\n",
    "                    key = (NonTerminal(next_s), 0.0)\n",
    "                    outcomes[key] = outcomes.get(key, 0.0) + p_survive\n",
    "\n",
    "        return Categorical(outcomes)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _next_state(\n",
    "        self,\n",
    "        s: DiscountState,\n",
    "        action: int,\n",
    "        purchased: FrozenSet[int],\n",
    "    ) -> DiscountState:\n",
    "        \"\"\"Deterministic next state given the current state, action, and\n",
    "        which products were purchased.\n",
    "\n",
    "        Args:\n",
    "            s:         Current (inner) DiscountState.\n",
    "            action:    Chosen action (0 = no promo, 1..N = promote product j).\n",
    "            purchased: Frozenset of 0-based indices of purchased products\n",
    "                       (empty if nothing was bought).\n",
    "        \"\"\"\n",
    "        # A. Churn propensity: decrease on any purchase, increase on idle\n",
    "        curr_c = self.C_GRID[s.churn_propensity]\n",
    "        if len(purchased) > 0:\n",
    "            new_c_val = max(curr_c - self.eta, 0.0)\n",
    "        else:\n",
    "            new_c_val = min(curr_c + self.eta, 1.0)\n",
    "        new_c_idx = int(np.argmin([abs(new_c_val - g) for g in self.C_GRID]))\n",
    "\n",
    "        # B. Per-product memory and recency\n",
    "        new_m: list[int] = []\n",
    "        new_l: list[int] = []\n",
    "        for j in range(self.N):\n",
    "            # Memory: exponential decay + bump if promoted\n",
    "            curr_m = self.M_GRID[s.discount_memory[j]]\n",
    "            is_promoted = (action == j + 1)\n",
    "            next_m_val = self.alpha * curr_m + (self.delta if is_promoted else 0.0)\n",
    "            new_m.append(int(np.argmin(\n",
    "                [abs(next_m_val - g) for g in self.M_GRID]\n",
    "            )))\n",
    "\n",
    "            # Recency: reset to 0 if bought, else increment\n",
    "            curr_l = self.L_GRID[s.purchase_recency[j]]\n",
    "            next_l_val = 0 if j in purchased else curr_l + 1\n",
    "            new_l.append(int(np.argmin(\n",
    "                [abs(next_l_val - g) for g in self.L_GRID]\n",
    "            )))\n",
    "\n",
    "        return DiscountState(\n",
    "            churn_propensity=new_c_idx,\n",
    "            discount_memory=tuple(new_m),\n",
    "            purchase_recency=tuple(new_l),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddec0ad",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Dunnhumby. (2017). *The Complete Journey: Household Analytics and Marketing Impact.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
